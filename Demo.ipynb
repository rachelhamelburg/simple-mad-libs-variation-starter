{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb08d785",
   "metadata": {},
   "source": [
    "In the Read Me file, I explained how the word_tokenizer tokenizes our text into words (more specifically, it tokenizes a string into a list of substrings). NLTK tokenizer and pos-tagger instantiate a list of tuples, 'tagged' (In this explanation, I've labeled it 'sample_tagged').  The tuples that the pos tagger produce are composed of this tokenized word, and its associated part of speech. Here is an example of how this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8dfc4629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized text ['The', 'dog', 'sleepily', 'walked', 'away', '.']\n",
      "Tagged text [('The', 'DT'), ('dog', 'NN'), ('sleepily', 'RB'), ('walked', 'VBD'), ('away', 'RB'), ('.', '.')]\n",
      "List type:  <class 'list'>\n",
      "List item type:  <class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sample_text = \"The dog sleepily walked away.\"\n",
    "sample_words = word_tokenize(sample_text)\n",
    "sample_tagged = nltk.pos_tag(sample_words)\n",
    "print('Tokenized text',sample_words)\n",
    "print('Tagged text',sample_tagged)\n",
    "print('List type: ', type(sample_tagged))\n",
    "print('List item type: ', type(sample_tagged[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db33da80",
   "metadata": {},
   "source": [
    "At first glance, one might ask why I did not change this list into a dictionary or create a dictionary. After all, after using the NLTK pos-tagged, many go on to change the data structure for further processing, like graph representations (for understanding semantic relationships), objects and classes (for N.E.R.), and more.  I originally thought that implementing a dictionary with key (pos) value (words) pairs would be the most efficient option for this project, and was hesitant at the thought of just replacing tuple items. However, the key to successful word-games like mad libs is maintaining word position in a sentence. This is why I decided to stick with a list. In a dictionary, when several values are associated with one key, they have to be stored in an iterable, like a list. You would have to iterate all the values of that list before moving to the next key (pos). This will disturb the order of the sentence. Have a look at the output in the cell below, which shows the comparison between the original text and the modified text if we use a dictionary when a sentence contains 2 adverbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5ab1ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dictionary:\n",
      "OrderedDict([('DT', ['The']), ('NN', ['dog']), ('RB', ['sleepily', 'away']), ('VBD', ['walked']), ('.', ['.'])])\n",
      "\n",
      "DT: The: The\n",
      "NN: dog: dog\n",
      "RB: sleepily: sleepily\n",
      "RB: away: away\n",
      "VBD: walked: walked\n",
      ".: .: .\n",
      "\n",
      "Modified Dictionary:\n",
      "\n",
      "OrderedDict([('DT', ['The']), ('NN', ['dog']), ('RB', ['sleepily', 'away']), ('VBD', ['walked']), ('.', ['.'])])\n",
      "\n",
      "Original Sentence: The dog sleepily walked away.\n",
      "\n",
      "Modified Sentence The dog sleepily away walked .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import OrderedDict\n",
    "\n",
    "sample_text = \"The dog sleepily walked away.\"\n",
    "sample_words = word_tokenize(sample_text)\n",
    "sample_tagged = nltk.pos_tag(sample_words)\n",
    "\n",
    "sample_dict = OrderedDict()\n",
    "for word, pos in sample_tagged:\n",
    "    if pos not in sample_dict:\n",
    "        sample_dict[pos] = [word]\n",
    "    else:\n",
    "        sample_dict[pos].append(word)\n",
    "\n",
    "print(\"Original Dictionary:\")\n",
    "print(sample_dict)\n",
    "print('')\n",
    "\n",
    "for pos_tag, words in sample_dict.items():\n",
    "    for i in range(len(words)):\n",
    "        new_word = input(f\"{pos_tag}: {words[i]}: \")\n",
    "        words[i] = new_word\n",
    "\n",
    "print(\"\\nModified Dictionary:\")\n",
    "print('')\n",
    "print(sample_dict)\n",
    "\n",
    "modified_sentence = ' '.join(word for words in sample_dict.values() for word in words)\n",
    "print('')\n",
    "print('Original Sentence:', sample_text)\n",
    "print('')\n",
    "print('Modified Sentence', modified_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67897eb9",
   "metadata": {},
   "source": [
    "On the other hand, with a list of tuples, the order of the words in every sentence is maintained. We can replace tuple items at particular indices with ease.\n",
    "\n",
    "After insantiating 'tagged', I made a list called pos_list containing the relevant pos's (those which I wanted to extract for my project implementation).  You'll notice below that I only chose 5 pos's - nouns, adjectives, verb-ings, past-tense verbs, and adverbs, respectively. I did so because Player 1 creating the story already allows for great variability in sentence structure. In short, it already has the potential to get pretty whacky, so I didn't want to implement too many possible changes at the expense of interpretability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98715110",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list_sample = ['NN', 'JJ', 'VBG', 'VBD', 'RB']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e71a416",
   "metadata": {},
   "source": [
    "The next step is to iterate through the list, the elements of the tuple, and the (index, (word, pos)) of 'tagged'. The application looks for pos's in 'tagged' list that match those in pos list. For those matches, we create user-friendly labels, so that the user is prompted with \"Noun, Adjective, etc.\", instead of Penn Treebank syntax: \"NN\", \"JJ\". The last line replaces the tuple at the appropriate index with their input and pos. This is another benefit of the list in Python, and another reason why I chose it - because it is mutable and dynamic. So, while we can't change the items within the tuple without replacing it entirely, we can at least add and remove tuples as we please."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc2a762e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "('The', 'DT')\n",
      "Noun: cat\n",
      "\n",
      "('cat', 'NN')\n",
      "Adverb: happily\n",
      "\n",
      "('happily', 'RB')\n",
      "Past-Tense Verb: pranced\n",
      "\n",
      "('pranced', 'VBD')\n",
      "Adverb: along\n",
      "\n",
      "('along', 'RB')\n",
      "\n",
      "('.', '.')\n"
     ]
    }
   ],
   "source": [
    "for i, (word_sample, pos_sample) in enumerate(sample_tagged):\n",
    "    if pos_sample in pos_list_sample:\n",
    "        label_sample = (\n",
    "            \"Noun\" if pos_sample == 'NN' else\n",
    "            \"Adverb\" if pos_sample == 'RB' else\n",
    "            \"Past-Tense Verb\" if pos_sample == 'VBD' else\n",
    "            \"Verb-ING\" if pos_sample == 'VBG' else\n",
    "            \"Unknown\"   \n",
    "        )\n",
    "        user_input_sample = input(f'{label_sample}: ')\n",
    "        sample_tagged[i] = (user_input_sample, pos_sample)\n",
    "    print(f'\\n{sample_tagged[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97ae01a",
   "metadata": {},
   "source": [
    "Then, all that is left to do is rebuild the text! This is yet another reason why the array is integral. We can use \"join\" to join a list's elements, separated by a space, which allows us to read the output as a sequence of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2be7110e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: The dog sleepily walked away.\n",
      "Modified Sentence: The cat happily pranced along .\n"
     ]
    }
   ],
   "source": [
    "reconstructed_sentence_sample = ' '.join([word_sample for word_sample, pos_sample in sample_tagged])\n",
    "print(\"Original Sentence:\", sample_text)\n",
    "print(\"Modified Sentence:\", reconstructed_sentence_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8bb8bf",
   "metadata": {},
   "source": [
    "As you can probably tell, the NLTK pos tagger is not perfect. It doesn't totally capture information about plurality, among other limitations. I think the integration of a LLM would be great for this project. You could have it generate stories based on an idea you have, and its understanding of pos's undoubtedly has the potential to be more robust."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
